{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from darts import TimeSeries, concatenate\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from darts.metrics import mae, mse, mape, rmse, smape\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from darts.models import RNNModel, BlockRNNModel, TCNModel, TransformerModel, TCNModel, NBEATSModel\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "\n",
    "from config import config\n",
    "\n",
    "freq='2H3T14S'\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"OpenMarsHyperOptFixedHorizon\")\n",
    "pd.options.plotting.backend = \"plotly\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(training_file, testing_file):\n",
    "    dataframes = []\n",
    "    for data_file in [training_file, testing_file]:\n",
    "        parser = lambda data_string: datetime.strptime(data_string, '%Y-%m-%d %H:%M:%S')\n",
    "        dataframe = pd.read_csv(data_file, parse_dates=['Time'],\n",
    "                                date_parser=parser)\n",
    "        print(f\"Rows in {data_file}: {len(dataframe)}\")\n",
    "        dataframe.drop(['Ls', 'LT', 'CO2ice'], axis=1, inplace=True)\n",
    "        dataframes.append(dataframe)\n",
    "\n",
    "    return pd.concat(dataframes, axis=0)\n",
    "\n",
    "\n",
    "def preprocess(dataframe):\n",
    "        time = pd.date_range(\"1998-07-15 21:23:39\", periods=len(dataframe), freq=freq)\n",
    "        dataframe.index = time\n",
    "        dataframe = dataframe.drop(['Time'], axis=1)\n",
    "        return dataframe\n",
    "\n",
    "def create_series(dataframe):\n",
    "        series = TimeSeries.from_dataframe(dataframe, time_col=None, value_cols=None, fill_missing_dates=True, freq=freq, fillna_value=None)\n",
    "        return series.astype(np.float32)\n",
    "\n",
    "def create_train_val_test_series(series):\n",
    "        train, temp = series.split_after(0.7)\n",
    "        val, test = temp.split_after(0.67)\n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27399/3478556548.py:5: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  dataframe = pd.read_csv(data_file, parse_dates=['Time'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in ../data/data_files/insight_openmars_training_time.csv: 72196\n",
      "Rows in ../data/data_files/insight_openmars_test_time.csv: 16364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27399/3478556548.py:5: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  dataframe = pd.read_csv(data_file, parse_dates=['Time'],\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88560"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = load_dataset('../data/data_files/insight_openmars_training_time.csv',\n",
    "                         '../data/data_files/insight_openmars_test_time.csv')\n",
    "dataframe = preprocess(dataframe)\n",
    "full_series = create_series(dataframe)\n",
    "train, val, test = create_train_val_test_series(full_series)\n",
    "# print(len(train), len(val), len(test))\n",
    "len(full_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tsurf', 'Psurf', 'cloud', 'vapour', 'u_wind', 'v_wind', 'dust',\n",
       "       'temp'],\n",
       "      dtype='object', name='component')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler()  # default uses sklearn's MinMaxScaler\n",
    "full_series = scaler.fit_transform(full_series)\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "lr_scheduler_cls = torch.optim.lr_scheduler.ExponentialLR\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61992 17800 8768\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_block_rnn_model():\n",
    "\n",
    "    # select input and output chunk lengths\n",
    "    in_len = 84\n",
    "    out_len =  12\n",
    "    batch_size = 96\n",
    "\n",
    "    # Other hyperparameters\n",
    "    n_rnn_layers = 2\n",
    "    hidden_dim =  30\n",
    "    dropout = 0.25\n",
    "    lr = 0.0005\n",
    "    # reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # pruner = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0008, patience=3, verbose=False)\n",
    "    callbacks = [early_stopper]\n",
    "\n",
    "    pl_trainer_kwargs = {\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"max_epochs\": 50,\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"callbacks\": callbacks,\n",
    "    }\n",
    "\n",
    "    common_model_args = {\n",
    "        \"optimizer_kwargs\": {'lr': lr},\n",
    "        \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "        \"lr_scheduler_cls\": lr_scheduler_cls,\n",
    "        \"lr_scheduler_kwargs\": config.lr_scheduler_kwargs,\n",
    "        \"likelihood\": None,  # use a likelihood for probabilistic forecasts\n",
    "        \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "        \"force_reset\": True,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "    }\n",
    "    # build the BlockRNNModel model\n",
    "    model = BlockRNNModel(model = \"LSTM\",\n",
    "                                input_chunk_length= in_len,\n",
    "                                output_chunk_length = out_len,\n",
    "                                n_rnn_layers = n_rnn_layers,\n",
    "                                hidden_dim = hidden_dim,\n",
    "                                batch_size = batch_size,\n",
    "                                model_name=\"BlockRNNModel_84_12\", \n",
    "                                dropout = dropout,\n",
    "                                **common_model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_tcn_model():\n",
    "    in_len = 84\n",
    "    out_len =  12\n",
    "    batch_size = 96\n",
    "    # Other hyperparameters\n",
    "    kernel_size = 2\n",
    "    num_filters = 6\n",
    "    weight_norm = True\n",
    "    dilation_base = 2\n",
    "    dropout = 0.05\n",
    "    lr = 0.0005\n",
    "    # reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "    # pruner = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0008, patience=3, verbose=False)\n",
    "    callbacks = [early_stopper]\n",
    "\n",
    "\n",
    "\n",
    "    pl_trainer_kwargs = {\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"max_epochs\": 50,\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"callbacks\": callbacks,\n",
    "    }\n",
    "\n",
    "    common_model_args = {\n",
    "        \"optimizer_kwargs\": {'lr': lr},\n",
    "        \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "        \"lr_scheduler_cls\": lr_scheduler_cls,\n",
    "        \"lr_scheduler_kwargs\": config.lr_scheduler_kwargs,\n",
    "        \"likelihood\": None,  # use a likelihood for probabilistic forecasts\n",
    "        \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "        \"force_reset\": True,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "    }\n",
    "\n",
    "    # build the TCN model\n",
    "    model = TCNModel(\n",
    "    input_chunk_length= in_len,\n",
    "    output_chunk_length = out_len,\n",
    "    dilation_base = dilation_base,\n",
    "    weight_norm = weight_norm,\n",
    "    kernel_size = kernel_size,\n",
    "    num_filters = num_filters,\n",
    "    model_name = 'TCNModel_84_12',\n",
    "    dropout = dropout,\n",
    "    **common_model_args\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_transformer_model():\n",
    "    in_len = 84\n",
    "    out_len =  12\n",
    "    batch_size = 96\n",
    "\n",
    "    # Other hyperparameters\n",
    "\n",
    "    d_model=12\n",
    "    nhead=6\n",
    "\n",
    "    num_encoder_layers=2\n",
    "    num_decoder_layers=4\n",
    "    dim_feedforward=64\n",
    "    dropout = 0.05\n",
    "    lr = 0.0005\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0008, patience=3, verbose=False)\n",
    "    callbacks = [early_stopper]\n",
    "\n",
    "\n",
    "\n",
    "    pl_trainer_kwargs = {\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"max_epochs\": 50,\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"callbacks\": callbacks,\n",
    "    }\n",
    "\n",
    "    common_model_args = {\n",
    "        \"optimizer_kwargs\": {'lr': lr},\n",
    "        \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "        \"lr_scheduler_cls\": lr_scheduler_cls,\n",
    "        \"lr_scheduler_kwargs\": config.lr_scheduler_kwargs,\n",
    "        \"likelihood\": None,  # use a likelihood for probabilistic forecasts\n",
    "        \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "        \"force_reset\": True,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "    }\n",
    "\n",
    "    # build the Transfofrmer model\n",
    "    model = TransformerModel(\n",
    "    input_chunk_length=in_len,\n",
    "    output_chunk_length=out_len,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    activation=\"relu\",\n",
    "    model_name = 'TransformerModel_84_12',\n",
    "    **common_model_args,\n",
    ")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_n_beats_model():\n",
    "    in_len = 84\n",
    "    out_len =  12\n",
    "    batch_size = 96\n",
    "    lr = 0.0005\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Other hyperparameters\n",
    "    num_blocks=3\n",
    "    num_layers=4\n",
    "    layer_widths=512\n",
    "    dropout = 0.05\n",
    "\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0008, patience=3, verbose=False)\n",
    "    callbacks = [early_stopper]\n",
    "\n",
    "    pl_trainer_kwargs = {\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"max_epochs\": 50,\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"callbacks\": callbacks,\n",
    "    }\n",
    "\n",
    "    common_model_args = {\n",
    "        \"optimizer_kwargs\": {'lr': lr},\n",
    "        \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "        \"lr_scheduler_cls\": lr_scheduler_cls,\n",
    "        \"lr_scheduler_kwargs\": config.lr_scheduler_kwargs,\n",
    "        \"likelihood\": None,  # use a likelihood for probabilistic forecasts\n",
    "        \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "        \"force_reset\": True,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"random_state\": 42,\n",
    "\n",
    "    }\n",
    "\n",
    "# build the NBeats model\n",
    "    model = NBEATSModel(\n",
    "        generic_architecture=False,\n",
    "        num_blocks=num_blocks,\n",
    "        num_layers=num_layers,\n",
    "        layer_widths=layer_widths,\n",
    "        loss_fn=torch.nn.MSELoss(),\n",
    "        input_chunk_length= in_len,\n",
    "        output_chunk_length = out_len,\n",
    "        model_name = 'NBEATSModel_84_12',\n",
    "        dropout=dropout,\n",
    "        **common_model_args,\n",
    "\n",
    ")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forecast(model, predict_after_series = val, variable  = 'dust', forecast_type = 'backtest', n = 84, forecast_horizon = 12):\n",
    "    result_accumulator = {}\n",
    "    print(f'For model {model.model_name}')\n",
    "    if forecast_type == 'backtest':\n",
    "        pred_series = model.historical_forecasts(series=full_series, \n",
    "                                        past_covariates=None,\n",
    "                                        future_covariates=None,\n",
    "                                        start = test.start_time(),\n",
    "                                        stride = forecast_horizon,\n",
    "                                        retrain=False,\n",
    "                                        verbose=False, \n",
    "                                        forecast_horizon=forecast_horizon,\n",
    "                                        last_points_only = False\n",
    "                                        )\n",
    "        pred_series = concatenate(pred_series)\n",
    "\n",
    "    else:\n",
    "        pred_series = model.predict(series=predict_after_series, n = n)\n",
    "    test_variable = test[variable]\n",
    "    pred_variable = pred_series[variable]\n",
    "    if variable != 'cloud':\n",
    "\n",
    "        result_accumulator[variable] = {\n",
    "            \"mae\": mae(test_variable, pred_variable),\n",
    "            \"mse\": mse(test_variable, pred_variable),\n",
    "            \"mape\": mape(test_variable, pred_variable),\n",
    "            \"rmse\": rmse(test_variable, pred_variable)\n",
    "        }\n",
    "    else:\n",
    "        result_accumulator[variable] = {\n",
    "            \"mae\": mae(test_variable, pred_variable),\n",
    "            \"mse\": mse(test_variable, pred_variable),\n",
    "            \"rmse\": rmse(test_variable, pred_variable)\n",
    "        }\n",
    "\n",
    "\n",
    "    return result_accumulator, pred_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(model, mlflow_run_id, test, pred_series, n = 84, type = 'backtest', variable = 'dust'):\n",
    "      all_artifacts = []\n",
    "      if type == 'backtest':\n",
    "            df_to_plot = pd.DataFrame({'Actual': scaler.inverse_transform(test)[variable].pd_series(), model.model_name.split('_')[0]: scaler.inverse_transform(pred_series)[variable].pd_series()})\n",
    "            fig = df_to_plot.plot(title=f\"{variable} {type}\", template=\"simple_white\",\n",
    "                  labels=dict(index=\"time\", value=variable, variable=\"Legend\"))\n",
    "            fig.show()\n",
    "            fig.write_image(f'../plots/{model.model_name}_{variable}_{type}.png')\n",
    "            # plt.savefig(f'../plots/{model.model_name}_{variable}_{type}.png')\n",
    "            all_artifacts.append(f'../plots/{model.model_name}_{variable}_{type}.png')\n",
    "\n",
    "            df_to_plot_zoom = pd.DataFrame({'Actual': scaler.inverse_transform(test)[variable][pd.Timestamp('2018-06-05T20:07:47'):pd.Timestamp('2018-07-31T11:09:27')].pd_series(), model.model_name.split('_')[0]: scaler.inverse_transform(pred_series)[variable][pd.Timestamp('2018-06-05T20:07:47'):pd.Timestamp('2018-07-31T11:09:27')].pd_series()})\n",
    "            fig2 = df_to_plot_zoom.plot(title=f\"Zoomed {variable} {type}\", template=\"simple_white\",\n",
    "                  labels=dict(index=\"time\", value=variable, variable=\"Legend\"))\n",
    "            fig2.show()\n",
    "            fig2.write_image(f'../plots/{model.model_name}_{variable}_{type}_zoomed.png')\n",
    "            # plt.savefig(f'../plots/{model.model_name}_{variable}_{type}_zoomed.png')\n",
    "            all_artifacts.append(f'../plots/{model.model_name}_{variable}_{type}_zoomed.png')\n",
    "      elif type == 'predict':\n",
    "            df_to_plot = pd.DataFrame({'Actual': scaler.inverse_transform(full_series[pd.Timestamp('2018-06-05T20:07:47'):])[variable].pd_series(), model.model_name.split('_')[0]: scaler.inverse_transform(pred_series)[variable].pd_series()})\n",
    "            fig = df_to_plot.plot(title=f\"{variable} {type}\", template=\"simple_white\",\n",
    "                  labels=dict(index=\"time\", value=variable, variable=\"Legend\"))\n",
    "            fig.show()\n",
    "            fig.write_image(f'../plots/{model.model_name}_{variable}_{type}.png')\n",
    "            # plt.savefig(f'../plots/{model.model_name}_{variable}_{type}.png')\n",
    "            all_artifacts.append(f'../plots/{model.model_name}_{variable}_{type}.png')\n",
    "\n",
    "            df_to_plot_zoom = pd.DataFrame({'Actual': scaler.inverse_transform(full_series)[variable][pd.Timestamp('2018-06-05T20:07:47'):pd.Timestamp('2018-07-31T11:09:27')].pd_series(), \n",
    "                                                model.model_name.split('_')[0]: scaler.inverse_transform(pred_series)[variable].pd_series()})\n",
    "            fig2 = df_to_plot_zoom.plot(title=f\"Zoomed {variable} {type}\", template=\"simple_white\",\n",
    "                  labels=dict(index=\"time\", value=variable, variable=\"Legend\"))\n",
    "            fig2.show()\n",
    "            fig2.write_image(f'../plots/{model.model_name}_{variable}_{type}_zoomed.png')\n",
    "            # plt.savefig(f'../plots/{model.model_name}_{variable}_{type}_zoomed.png')\n",
    "            all_artifacts.append(f'../plots/{model.model_name}_{variable}_{type}_zoomed.png')\n",
    "\n",
    "      return all_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models =    {'BlockRNNModel_84_12': create_block_rnn_model(),\n",
    "#                 'TCNModel_84_12': create_tcn_model(),\n",
    "#                 'TransformerModel_84_12': create_transformer_model(),\n",
    "#                 'NBEATSModel_84_12': create_n_beats_model(),\n",
    "#                   }\n",
    "\n",
    "# all_models =    {'BlockRNNModel_84_12': BlockRNNModel.load_from_checkpoint(model_name = \"BlockRNNModel_84_12\", best=True),\n",
    "#                 'TCNModel_84_12': TCNModel.load_from_checkpoint(model_name =\"TCNModel_84_12\", best=True),\n",
    "#                 'TransformerModel_84_12': TransformerModel.load_from_checkpoint(model_name =\"TransformerModel_84_12\", best=True),\n",
    "#                 'NBEATSModel_84_12': NBEATSModel.load_from_checkpoint(model_name =\"NBEATSModel_84_12\", best=True),\n",
    "#                   }\n",
    "\n",
    "# for model_name, model in all_models.items():\n",
    "#         # detect if a GPU is available\n",
    "#     if torch.cuda.is_available():\n",
    "#         num_workers = 8\n",
    "#     else:\n",
    "#         num_workers = 0\n",
    "\n",
    "#     series = create_series(dataframe)\n",
    "\n",
    "#     # when validating during training, we can use a slightly longer validation\n",
    "#     # set which also contains the first input_chunk_length time steps\n",
    "#     model_val_set = val\n",
    "\n",
    "#     # train the model\n",
    "#     model.fit(\n",
    "#         series=train,\n",
    "#         val_series=model_val_set,\n",
    "#         num_loader_workers=num_workers,\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BlockRNNModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_models \u001b[38;5;241m=\u001b[39m    {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlockRNNModel_84_12_HyperOpt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mBlockRNNModel\u001b[49m\u001b[38;5;241m.\u001b[39mload_from_checkpoint(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlockRNNModel_84_12_HyperOpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTCNModel_84_12_HyperOpt\u001b[39m\u001b[38;5;124m'\u001b[39m: TCNModel\u001b[38;5;241m.\u001b[39mload_from_checkpoint(model_name \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTCNModel_84_12_HyperOpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformerModel_84_12_HyperOpt\u001b[39m\u001b[38;5;124m'\u001b[39m: TransformerModel\u001b[38;5;241m.\u001b[39mload_from_checkpoint(model_name \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformerModel_84_12_HyperOpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNBEATSModel_84_12_HyperOpt_HyperOpt\u001b[39m\u001b[38;5;124m'\u001b[39m: NBEATSModel\u001b[38;5;241m.\u001b[39mload_from_checkpoint(model_name \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNBEATSModel_84_12_HyperOpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m                   }\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# trained_models =    {\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#                 'NBEATSModel_84_12_HyperOpt': NBEATSModel.load_from_checkpoint(model_name =\"NBEATSModel_84_12_HyperOpt\", best=True),\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#                   }\u001b[39;00m\n\u001b[1;32m     11\u001b[0m all_variables \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTsurf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPsurf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloud\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvapour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu_wind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_wind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BlockRNNModel' is not defined"
     ]
    }
   ],
   "source": [
    "trained_models =    {'BlockRNNModel_84_12_HyperOpt': BlockRNNModel.load_from_checkpoint(model_name = \"BlockRNNModel_84_12_HyperOpt\", best=True),\n",
    "                'TCNModel_84_12_HyperOpt': TCNModel.load_from_checkpoint(model_name =\"TCNModel_84_12_HyperOpt\", best=True),\n",
    "                'TransformerModel_84_12_HyperOpt': TransformerModel.load_from_checkpoint(model_name =\"TransformerModel_84_12_HyperOpt\", best=True),\n",
    "                'NBEATSModel_84_12_HyperOpt_HyperOpt': NBEATSModel.load_from_checkpoint(model_name =\"NBEATSModel_84_12_HyperOpt\", best=True),\n",
    "                  }\n",
    "# trained_models =    {\n",
    "#                 'NBEATSModel_84_12_HyperOpt': NBEATSModel.load_from_checkpoint(model_name =\"NBEATSModel_84_12_HyperOpt\", best=True),\n",
    "#                   }\n",
    "\n",
    "\n",
    "all_variables = ['dust', 'Tsurf', 'Psurf', 'cloud', 'vapour', 'u_wind', 'v_wind', 'temp']\n",
    "all_variables = ['dust']\n",
    "\n",
    "types = ['backtest', 'predict']\n",
    "for model_name, model in trained_models.items():\n",
    "    with mlflow.start_run(run_name=model.model_name) as run:\n",
    "        all_artifacts = []\n",
    "        mlflow.set_tag(\"Model_Name\", model.model_name)\n",
    "        mlflow.log_params(model.model_params)\n",
    "        for variable in all_variables:\n",
    "            \n",
    "            for type in types:\n",
    "                metrics = {}\n",
    "                if type == 'predict':\n",
    "                    series = full_series[:pd.Timestamp('2018-06-05T20:07:47')]\n",
    "                    eval_results, pred_series = forecast(model, predict_after_series=series,\n",
    "                                                          variable=variable, forecast_type=type)\n",
    "                    metrics[type] = eval_results\n",
    "                    # mlflow.log_artifact(metrics)\n",
    "                    \n",
    "                else:\n",
    "                    series = test\n",
    "                    eval_results, pred_series = forecast(model, predict_after_series=series,\n",
    "                                                          variable=variable, forecast_type=type)\n",
    "                    metrics[type] = eval_results\n",
    "                    # mlflow.log_artifact(metrics)\n",
    "                print(eval_results)\n",
    "                if type == 'backtest':\n",
    "                    for var, metric_dict in eval_results.items():\n",
    "                        mlflow.log_metrics(metric_dict)\n",
    "                        # for metric_name, val in metric_dict.items():\n",
    "                        #     mlflow.log_metrics(metric)\n",
    "                mlflow.log_table(data = eval_results, artifact_file = f'metrics_{model.model_name}_{variable}_{type}.json')\n",
    "                artifacts = plotting(model, run.info.run_id, test, pred_series, type = type,  variable = variable)\n",
    "                all_artifacts.extend(artifacts)\n",
    "                columns = ['Tsurf', 'Psurf', 'cloud', 'vapour', 'u_wind', 'v_wind', 'dust', 'temp']\n",
    "                predict_df = scaler.inverse_transform(pred_series).pd_dataframe()\n",
    "                predicted_file_nm =f'{model.model_name}_{type}'\n",
    "                final_path = f'/home/ubuntu/OpenMarsML/data/predicted_data/{predicted_file_nm}.csv'\n",
    "                print(f'Going to save predictions of model {model.model_name} to file {final_path}')\n",
    "                predict_df.to_csv(final_path)\n",
    "\n",
    "        for artifact in all_artifacts:\n",
    "            print(artifact)\n",
    "            mlflow.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_to_predict = full_series[:pd.Timestamp('2018-06-05T20:07:47')]\n",
    "# series_predicted_actual = full_series[:pd.Timestamp('2018-06-05 22:11:01')]\n",
    "# final_path = f'/home/ubuntu/OpenMarsML/data/data_files/series_to_predict.csv'\n",
    "# series_to_predict.to_csv(f'/home/ubuntu/OpenMarsML/data/data_files/series_to_predict.csv')\n",
    "# series_predicted_actual.to_csv(f'/home/ubuntu/OpenMarsML/data/data_files/predicted_series_actual_vals.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
